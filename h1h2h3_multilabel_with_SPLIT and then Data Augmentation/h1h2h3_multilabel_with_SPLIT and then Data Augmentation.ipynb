{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, RobertaModel, AdamW\n",
    "import nlpaug.augmenter.word as naw\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np  # Add this import statement to include NumPy\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if the required columns exist\n",
    "        if 'text' not in data.columns or len(data.columns) < 2:\n",
    "            raise ValueError(\"CSV file does not contain 'text' and label columns.\")\n",
    "\n",
    "        texts = data['text'].tolist()\n",
    "        labels = data.iloc[:, 1:].values.tolist()\n",
    "        \n",
    "        return texts, labels\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data from {file_path}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# Example usage:\n",
    "file_path_h1 = \"model1.csv\"\n",
    "file_path_h2 = \"model2.csv\"\n",
    "texts_h1, labels_h1 = load_data(file_path_h1)\n",
    "texts_h2, labels_h2 = load_data(file_path_h2)\n",
    "print(len(texts_h1))\n",
    "print(len(texts_h2))\n",
    "print(len(labels_h1))\n",
    "print(len(labels_h2))\n",
    "\n",
    "def split_data(texts, labels, train_ratio, val_ratio, test_ratio):\n",
    "    total_samples = len(texts)\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Ratios must add up to 1.0\"\n",
    "\n",
    "    train_size = int(total_samples * train_ratio)\n",
    "    val_size = int(total_samples * val_ratio)\n",
    "    test_size = int(total_samples * test_ratio)\n",
    "\n",
    "    train_texts = texts[:train_size]\n",
    "    val_texts = texts[train_size:train_size + val_size]\n",
    "    test_texts = texts[train_size + val_size:]\n",
    "\n",
    "    train_labels = labels[:train_size]\n",
    "    val_labels = labels[train_size:train_size + val_size]\n",
    "    test_labels = labels[train_size + val_size:]\n",
    "\n",
    "    return train_texts, val_texts, test_texts, train_labels, val_labels, test_labels\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels_h1, labels_h2, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels_h1 = labels_h1\n",
    "        self.labels_h2 = labels_h2\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels_h1 = self.labels_h1[idx]\n",
    "        labels_h2 = self.labels_h2[idx]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'ids': input_ids,\n",
    "            'mask': attention_mask,\n",
    "            'labels_h1': torch.FloatTensor(labels_h1),  # Assuming labels are in a multi-label format\n",
    "            'labels_h2': torch.FloatTensor(labels_h2)\n",
    "        }\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Split the data into training, validation, and test sets for H1 and H2\n",
    "train_texts_h1, val_texts_h1, test_texts_h1, train_labels_h1, val_labels_h1, test_labels_h1 = split_data(texts_h1, labels_h1, 0.7, 0.1, 0.2)\n",
    "train_texts_h2, val_texts_h2, test_texts_h2, train_labels_h2, val_labels_h2, test_labels_h2 = split_data(texts_h2, labels_h2, 0.7, 0.1, 0.2)\n",
    "print(\"Length of train_texts_h1 before augmentation:\", len(train_texts_h1))\n",
    "print(\"Length of train_texts_h2 before augmentation:\", len(train_texts_h2))\n",
    "\n",
    "# Ensure the number of samples is the same for H1 and H2\n",
    "assert len(train_texts_h1) == len(train_texts_h2), \"Training data sizes do not match\"\n",
    "assert len(val_texts_h1) == len(val_texts_h2), \"Validation data sizes do not match\"\n",
    "assert len(test_texts_h1) == len(test_texts_h2), \"Test data sizes do not match\"\n",
    "\n",
    "print(\"H1 Data Split Sizes:\")\n",
    "print(f\"Training data size for H1: {len(train_texts_h1)}\")\n",
    "print(f\"Validation data size for H1: {len(val_texts_h1)}\")\n",
    "print(f\"Test data size for H1: {len(test_texts_h1)}\")\n",
    "\n",
    "print(\"\\nH2 Data Split Sizes:\")\n",
    "print(f\"Training data size for H2: {len(train_texts_h2)}\")\n",
    "print(f\"Validation data size for H2: {len(val_texts_h2)}\")\n",
    "print(f\"Test data size for H2: {len(test_texts_h2)}\")\n",
    "\n",
    "print(f\"Training data size for labels of H1: {len(train_labels_h1)}\")\n",
    "print(f\"Validating data size for labels of H1: {len(val_labels_h1)}\")\n",
    "print(f\"Testing data size for labels of H1: {len(test_labels_h1)}\")\n",
    "\n",
    "print(f\"Training data size for labels of H2: {len(train_labels_h2)}\")\n",
    "print(f\"Validating data size for labels of H2: {len(val_labels_h2)}\")\n",
    "print(f\"Testing data size for labels of H2: {len(test_labels_h2)}\")\n",
    "\n",
    "# Define a function for data augmentation\n",
    "def augment_data(texts, labels, num_augmentations=3):\n",
    "    aug = naw.SynonymAug(aug_src='wordnet')\n",
    "    augmented_texts = []\n",
    "    augmented_labels = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        augmented_samples = [aug.augment(text) for _ in range(num_augmentations)]\n",
    "        augmented_texts.extend(augmented_samples)\n",
    "        augmented_labels.extend([label] * num_augmentations)  # Use the same label for augmented samples\n",
    "    return augmented_texts, augmented_labels\n",
    "\n",
    "# Apply data augmentation to both H1 and H2 training data\n",
    "num_augmentations = 3\n",
    "augmented_train_texts_h1, augmented_train_labels_h1 = augment_data(train_texts_h1, train_labels_h1, num_augmentations)\n",
    "augmented_train_texts_h2, augmented_train_labels_h2 = augment_data(train_texts_h2, train_labels_h2, num_augmentations)\n",
    "print(\"Length of train_texts_h1 after augmentation:\", len(augmented_train_texts_h1))\n",
    "print(\"Length of train_texts_h2 after augmentation:\", len(augmented_train_texts_h2))\n",
    "\n",
    "# Combine the original data with augmented data\n",
    "train_texts_h1 = train_texts_h1 + augmented_train_texts_h1\n",
    "train_labels_h1 = train_labels_h1 + augmented_train_labels_h1\n",
    "\n",
    "train_texts_h2 = train_texts_h2 + augmented_train_texts_h2\n",
    "train_labels_h2 = train_labels_h2 + augmented_train_labels_h2\n",
    "\n",
    "print(\"Length of train_texts_h1 combined:\", len(train_texts_h1))\n",
    "print(\"Length of train_texts_h2 combined:\", len(train_texts_h2))\n",
    "\n",
    "print(\"Length of train_labels_h1 combined:\", len(train_labels_h1))\n",
    "print(\"Length of train_labels_h2 combined:\", len(train_labels_h2))\n",
    "\n",
    "# Create dataloaders for H1 and H2 using the augmented training data\n",
    "train_dataset_h1 = TextDataset(train_texts_h1, train_labels_h1, train_labels_h2, tokenizer)\n",
    "train_dataset_h2 = TextDataset(train_texts_h2, train_labels_h1, train_labels_h2, tokenizer)\n",
    "\n",
    "# Create dataloaders for validation and test data\n",
    "val_dataset_h1 = TextDataset(val_texts_h1, val_labels_h1, val_labels_h2, tokenizer)\n",
    "test_dataset_h1 = TextDataset(test_texts_h1, test_labels_h1, test_labels_h2, tokenizer)\n",
    "\n",
    "val_dataset_h2 = TextDataset(val_texts_h2, val_labels_h1, val_labels_h2, tokenizer)\n",
    "test_dataset_h2 = TextDataset(test_texts_h2, test_labels_h1, test_labels_h2, tokenizer)\n",
    "\n",
    "# Create dataloaders for H1 and H2\n",
    "train_dataloader_h1 = DataLoader(train_dataset_h1, batch_size=32, shuffle=True)\n",
    "train_dataloader_h2 = DataLoader(train_dataset_h2, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataloader_h1 = DataLoader(val_dataset_h1, batch_size=32, shuffle=False)\n",
    "val_dataloader_h2 = DataLoader(val_dataset_h2, batch_size=32, shuffle=False)\n",
    "\n",
    "test_dataloader_h1 = DataLoader(test_dataset_h1, batch_size=32, shuffle=False)\n",
    "test_dataloader_h2 = DataLoader(test_dataset_h2, batch_size=32, shuffle=False)\n",
    "\n",
    "print(len(train_dataloader_h1))\n",
    "print(len(train_dataloader_h2))\n",
    "print(len(val_dataloader_h1))\n",
    "print(len(val_dataloader_h2))\n",
    "print(len(test_dataloader_h1))\n",
    "print(len(test_dataloader_h2))\n",
    "\n",
    "# Define the number of labels for H1 and H2\n",
    "num_labels_h1 = 8  # Replace with the actual number of labels for H1\n",
    "num_labels_h2 = 32  # Replace with the actual number of labels for H2\n",
    "\n",
    "# Define the multi-label classifier model\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, num_labels_h1, num_labels_h2):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier_h1 = nn.Linear(self.roberta.config.hidden_size, num_labels_h1)\n",
    "        self.classifier_h2 = nn.Linear(self.roberta.config.hidden_size, num_labels_h2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids, attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        pooled_output = sequence_output.mean(dim=1)\n",
    "        logits_h1 = self.classifier_h1(pooled_output)\n",
    "        logits_h2 = self.classifier_h2(pooled_output)\n",
    "        return logits_h1, logits_h2\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 1\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create the model and move it to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_h1 = MultiLabelClassifier(num_labels_h1, num_labels_h2).to(device)\n",
    "model_h2 = MultiLabelClassifier(num_labels_h1, num_labels_h2).to(device)\n",
    "\n",
    "# Define optimizers and loss functions for H1 and H2 with weights\n",
    "optimizer_h1 = AdamW(model_h1.parameters(), lr=1e-5)\n",
    "loss_weight_h1 = 0.25  # Weight for H1\n",
    "loss_fn_h1 = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_h2 = AdamW(model_h2.parameters(), lr=1e-5)\n",
    "loss_weight_h2 = 0.75  # Weight for H2\n",
    "loss_fn_h2 = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Define a validation function\n",
    "def validate_model(model, val_dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['ids'].to(device)\n",
    "            attention_mask = batch['mask'].to(device)\n",
    "            labels_h1 = batch['labels_h1'].to(device)\n",
    "            labels_h2 = batch['labels_h2'].to(device)\n",
    "\n",
    "            logits_h1, logits_h2 = model(input_ids, attention_mask)\n",
    "            loss_h1 = loss_fn(logits_h1, labels_h1)\n",
    "            loss_h2 = loss_fn(logits_h2, labels_h2)\n",
    "            loss = (loss_weight_h1 * loss_h1) + (loss_weight_h2 * loss_h2)  # Weighted combination\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_loss / len(val_dataloader)\n",
    "    return avg_val_loss\n",
    "\n",
    "# Training loop for H1 and H2 with weights\n",
    "def train_model(model, dataloader, optimizer, loss_fn, device, num_epochs, val_dataloader):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['ids'].to(device)\n",
    "            attention_mask = batch['mask'].to(device)\n",
    "            labels_h1 = batch['labels_h1'].to(device)\n",
    "            labels_h2 = batch['labels_h2'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits_h1, logits_h2 = model(input_ids, attention_mask)\n",
    "            loss_h1 = loss_fn(logits_h1, labels_h1)\n",
    "            loss_h2 = loss_fn(logits_h2, labels_h2)\n",
    "            loss = (loss_weight_h1 * loss_h1) + (loss_weight_h2 * loss_h2)  # Weighted combination\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_train_loss = total_loss / len(dataloader)\n",
    "\n",
    "        # Calculate the validation loss for H1 and H2\n",
    "        val_loss = validate_model(model, val_dataloader, loss_fn, device)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Training and validation for H1 and H2 with weights\n",
    "train_model(model_h1, train_dataloader_h1, optimizer_h1, loss_fn_h1, device, num_epochs, val_dataloader_h1)\n",
    "train_model(model_h2, train_dataloader_h2, optimizer_h2, loss_fn_h2, device, num_epochs, val_dataloader_h2)\n",
    "\n",
    "# Evaluation loop for H1 and H2\n",
    "# Evaluation loop for H1\n",
    "def evaluate_h1(model_h1, test_dataloader_h1, test_labels_h1, device):\n",
    "    model_h1.eval()\n",
    "    all_preds_h1 = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_h1 in test_dataloader_h1:\n",
    "            input_ids_h1 = batch_h1['ids'].to(device)\n",
    "            attention_mask_h1 = batch_h1['mask'].to(device)\n",
    "\n",
    "            logits_h1, _ = model_h1(input_ids_h1, attention_mask_h1)\n",
    "            preds_h1 = torch.sigmoid(logits_h1).cpu().numpy()\n",
    "            all_preds_h1.append(preds_h1)\n",
    "\n",
    "    all_preds_h1 = np.vstack(all_preds_h1)\n",
    "    true_labels_h1 = test_labels_h1  # Replace with the actual true labels\n",
    "\n",
    "    # Define a threshold for classifying binary labels (you may need to adjust this threshold)\n",
    "    threshold = 0.5\n",
    "    binary_preds_h1 = (all_preds_h1 > threshold).astype(int)\n",
    "\n",
    "    # Calculate evaluation metrics for H1\n",
    "    accuracy_h1 = accuracy_score(true_labels_h1, binary_preds_h1)\n",
    "    precision_h1 = precision_score(true_labels_h1, binary_preds_h1, average='weighted')\n",
    "    recall_h1 = recall_score(true_labels_h1, binary_preds_h1, average='weighted')\n",
    "    f1_score_h1 = f1_score(true_labels_h1, binary_preds_h1, average='weighted')\n",
    "\n",
    "    print(\"\\nEvaluation Metrics for H1:\")\n",
    "    print(f\"Accuracy: {accuracy_h1:.4f}\")\n",
    "    print(f\"Precision: {precision_h1:.4f}\")\n",
    "    print(f\"Recall: {recall_h1:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score_h1:.4f}\")\n",
    "\n",
    "# Evaluation loop for H2\n",
    "def evaluate_h2(model_h2, test_dataloader_h2, test_labels_h2, device):\n",
    "    model_h2.eval()\n",
    "    all_preds_h2 = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_h2 in test_dataloader_h2:\n",
    "            input_ids_h2 = batch_h2['ids'].to(device)\n",
    "            attention_mask_h2 = batch_h2['mask'].to(device)\n",
    "\n",
    "            _, logits_h2 = model_h2(input_ids_h2, attention_mask_h2)\n",
    "            preds_h2 = torch.sigmoid(logits_h2).cpu().numpy()\n",
    "            all_preds_h2.append(preds_h2)\n",
    "\n",
    "    all_preds_h2 = np.vstack(all_preds_h2)\n",
    "    true_labels_h2 = test_labels_h2  # Replace with the actual true labels\n",
    "\n",
    "    # Define a threshold for classifying binary labels (you may need to adjust this threshold)\n",
    "    threshold = 0.5\n",
    "    binary_preds_h2 = (all_preds_h2 > threshold).astype(int)\n",
    "\n",
    "    # Calculate evaluation metrics for H2\n",
    "    accuracy_h2 = accuracy_score(true_labels_h2, binary_preds_h2)\n",
    "    precision_h2 = precision_score(true_labels_h2, binary_preds_h2, average='weighted')\n",
    "    recall_h2 = recall_score(true_labels_h2, binary_preds_h2, average='weighted')\n",
    "    f1_score_h2 = f1_score(true_labels_h2, binary_preds_h2, average='weighted')\n",
    "\n",
    "    print(\"\\nEvaluation Metrics for H2:\")\n",
    "    print(f\"Accuracy: {accuracy_h2:.4f}\")\n",
    "    print(f\"Precision: {precision_h2:.4f}\")\n",
    "    print(f\"Recall: {recall_h2:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score_h2:.4f}\")\n",
    "\n",
    "# Call the evaluation functions separately for H1 and H2\n",
    "evaluate_h1(model_h1, test_dataloader_h1, test_labels_h1, device)\n",
    "evaluate_h2(model_h2, test_dataloader_h2, test_labels_h2, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
